# %% [markdown]
# # Emotion Recognition in Greek Speech Using Wav2Vec 2.0

# %% [markdown]
# **Wav2Vec 2.0** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2`s ability to learn speech representations that are useful across multiple languages.
# 
# Similar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http://jalammar.github.io/illustrated-bert/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.
# 
# The authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf).
# 
# ---
# 
# **Wav2Vec 2.0** æ˜¯ä¸€ä¸ªç”¨äºŽè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œç”± Alexei Baevskiã€Michael Auli å’Œ Alex Conneau äºŽ [2020 å¹´ 9 æœˆ](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) å‘å¸ƒã€‚åœ¨ Wav2Vec2 åœ¨è‹±è¯­ ASR æ•°æ®é›† LibriSpeech ä¸Šå±•ç¤ºå‡ºå“è¶Šæ€§èƒ½åŽä¸ä¹…ï¼Œ*Facebook AI* æå‡ºäº† XLSR-Wav2Vec2ï¼ˆç‚¹å‡»[è¿™é‡Œ](https://arxiv.org/abs/2006.13979)ï¼‰ã€‚XLSR ä»£è¡¨*è·¨è¯­è¨€è¯­éŸ³è¡¨ç¤º*ï¼ŒæŒ‡çš„æ˜¯ XLSR-Wav2Vec2 å­¦ä¹ å¯¹å¤šç§è¯­è¨€æœ‰ç”¨çš„è¯­éŸ³è¡¨ç¤ºçš„èƒ½åŠ›ã€‚
# 
# ä¸Ž Wav2Vec2 ç±»ä¼¼ï¼ŒXLSR-Wav2Vec2 ä»Žè¶…è¿‡ 50 ç§è¯­è¨€çš„æ•°åä¸‡å°æ—¶æœªæ ‡è®°è¯­éŸ³ä¸­å­¦ä¹ å¼ºå¤§çš„è¯­éŸ³è¡¨ç¤ºã€‚ç±»ä¼¼äºŽ [BERT çš„æŽ©ç è¯­è¨€æ¨¡åž‹](http://jalammar.github.io/illustrated-bert/)ï¼Œè¯¥æ¨¡åž‹é€šè¿‡åœ¨å°†ç‰¹å¾å‘é‡ä¼ é€’ç»™ transformer ç½‘ç»œä¹‹å‰éšæœºæŽ©ç ç‰¹å¾å‘é‡æ¥å­¦ä¹ ä¸Šä¸‹æ–‡åŒ–çš„è¯­éŸ³è¡¨ç¤ºã€‚
# 
# ![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)
# 
# ä½œè€…é¦–æ¬¡å±•ç¤ºäº†åœ¨è·¨è¯­è¨€æœªæ ‡è®°è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒçš„ ASR æ¨¡åž‹ï¼ŒéšåŽåœ¨éžå¸¸å°‘çš„æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œè¯­è¨€ç‰¹å®šçš„å¾®è°ƒï¼Œèƒ½å¤Ÿå®žçŽ°æœ€å…ˆè¿›çš„ç»“æžœã€‚è¯·å‚è§å®˜æ–¹[è®ºæ–‡](https://arxiv.org/pdf/2006.13979.pdf)çš„è¡¨ 1-5ã€‚

# %% [markdown]
# During fine-tuning week hosted by HuggingFace, more than 300 people participated in tuning XLSR-Wav2Vec2's pretrained on low-resources ASR dataset for more than 50 languages. This model is fine-tuned using [Connectionist Temporal Classification](https://distill.pub/2017/ctc/) (CTC), an algorithm used to train neural networks for sequence-to-sequence problems and mainly in Automatic Speech Recognition and handwriting recognition. Follow this [notebook](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLSR_Wav2Vec2_on_Turkish_ASR_with_%F0%9F%A4%97_Transformers.ipynb#scrollTo=Gx9OdDYrCtQ1) for more information about XLSR-Wav2Vec2 fine-tuning.
# 
# This model was shown significant results in many low-resources languages. You can see the [competition board](https://paperswithcode.com/dataset/common-voice) or even testing the models from the [HuggingFace hub](https://huggingface.co/models?filter=xlsr-fine-tuning-week).
# 
# 
# In this notebook, we will go through how to use this model to recognize the emotional aspects of speech in a language (or even as a general view using for every classification problem). Before going any further, we need to install some handy packages and define some enviroment values.
# 
# ---
# 
# åœ¨ HuggingFace ä¸»åŠžçš„å¾®è°ƒå‘¨æœŸé—´ï¼Œè¶…è¿‡ 300 äººå‚ä¸Žäº† XLSR-Wav2Vec2 åœ¨ä½Žèµ„æºåž‹ ASR æ•°æ®é›†ä¸Šé’ˆå¯¹ 50 å¤šç§è¯­è¨€çš„é¢„è®­ç»ƒã€‚è¯¥æ¨¡åž‹ä½¿ç”¨ Connectionist Temporal Classification ï¼ˆCTCï¼‰ è¿›è¡Œå¾®è°ƒï¼ŒCTC æ˜¯ä¸€ç§ç”¨äºŽè®­ç»ƒç¥žç»ç½‘ç»œè§£å†³åºåˆ—åˆ°åºåˆ—é—®é¢˜çš„ç®—æ³•ï¼Œä¸»è¦ç”¨äºŽè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œæ‰‹å†™è¯†åˆ«ã€‚è¯·å…³æ³¨æ­¤ç¬”è®°æœ¬ï¼Œäº†è§£æœ‰å…³ XLSR-Wav2Vec2 å¾®è°ƒçš„æ›´å¤šä¿¡æ¯ã€‚
# 
# è¯¥æ¨¡åž‹åœ¨è®¸å¤šèµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæžœã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æ¯”èµ›æ¿ï¼Œç”šè‡³å¯ä»¥ä»Ž HuggingFace ä¸­å¿ƒæµ‹è¯•æ¨¡åž‹ã€‚
# 
# åœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¨¡åž‹æ¥è¯†åˆ«è¯­è¨€ä¸­è¯­éŸ³çš„æƒ…æ„Ÿæ–¹é¢ï¼ˆç”šè‡³ä½œä¸ºæ¯ä¸ªåˆ†ç±»é—®é¢˜çš„ä¸€èˆ¬è§†å›¾ï¼‰ã€‚åœ¨è¿›ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€äº›æ–¹ä¾¿çš„åŒ…å¹¶å®šä¹‰ä¸€äº› enviroment å€¼ã€‚
# 

# %%
## ç›´æŽ¥å‚è€ƒä¸Šé¢çš„æ ¼å¼æå–æ•°æ®äº†ï¼

### è¯»å– datasets/data.json
import json

with open("datasets/data.json", "r") as f:
    data = json.load(f)

train_data = []
valid_data = []
test_data = []

for item in data:
    if item["Split"] == "train":
        train_data.append(item)
    elif item["Split"] == "dev":
        valid_data.append(item)
    else: test_data.append(item)

# %%
import pandas as pd

train_df = pd.DataFrame(train_data)
valid_df = pd.DataFrame(valid_data)
test_df  = pd.DataFrame(test_data)

len(valid_df), len(train_df)

# %% [markdown]
# Let's explore how many labels (emotions) are in the dataset with what distribution.

# %%
print("Labels: ", train_df["PHQ8_Binary"].unique())
print()
train_df.groupby("PHQ8_Binary").count()["Unique_ID"]

# %% [markdown]
# Let's display some random sample of the dataset and run it a couple of times to get a feeling for the audio and the emotional label.

# %%
import torchaudio
import librosa
import IPython.display as ipd
import numpy as np

idx = np.random.randint(0, len(train_df))
sample = train_df.iloc[idx]
path = sample["Audio_Path"]
label = sample["PHQ8_Binary"]


print(f"ID Location: {idx}")
print(f"      Label: {label}")
print()

speech, sr = torchaudio.load(path)
speech = speech[0].numpy().squeeze()
ipd.Audio(data=np.asarray(speech), autoplay=True, rate=16000)

# %%
valid_df

# %% [markdown]
# For training purposes, we need to split data into train test sets; in this specific example, we break with a `20%` rate for the test set.

# %%
save_path = "datasets/output_data"

# ä»…ä¿ç•™Unique_ID	Speaker_ID	Inner_ID	Audio_Path	Audio_Length	Gender	PHQ8_Binary	PHQ8_Score å±žæ€§

train_df = train_df[["Unique_ID", "Audio_Path", "PHQ8_Binary", "PHQ8_Score"]]
valid_df = valid_df[["Unique_ID", "Audio_Path", "PHQ8_Binary", "PHQ8_Score"]]

train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)

train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
valid_df.to_csv(f"{save_path}/valid.csv", sep="\t", encoding="utf-8", index=False)


print(train_df.shape)
print(valid_df.shape)

# %% [markdown]
# ## Prepare Data for Training

# %%
# Loading the created dataset using datasets
from datasets import load_dataset, DownloadMode


data_files = {
    "train": "datasets/output_data/train.csv",
    "validation": "datasets/output_data/valid.csv",
}

dataset = load_dataset("csv", data_files=data_files, delimiter="\t", download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)
train_dataset = dataset["train"]
eval_dataset = dataset["validation"]

print(train_dataset)
print(eval_dataset)

# %%
# We need to specify the input and output column
input_column = 'Audio_Path'
output_column = 'PHQ8_Binary'

# %%
# we need to distinguish the unique labels in our SER dataset
label_list = train_dataset.unique(output_column)
label_list.sort()  # Let's sort it for determinism
num_labels = len(label_list)
print(f"A classification problem with {num_labels} classes: {label_list}")

# %% [markdown]
# In order to preprocess the audio into our classification model, we need to set up the relevant Wav2Vec2 assets regarding our language in this case `lighteternal/wav2vec2-large-xlsr-53-greek` fine-tuned by [Dimitris Papadopoulos](https://huggingface.co/lighteternal/wav2vec2-large-xlsr-53-greek). To handle the context representations in any audio length we use a merge strategy plan (pooling mode) to concatenate that 3D representations into 2D representations.
# 
# There are three merge strategies `mean`, `sum`, and `max`. In this example, we achieved better results on the mean approach. In the following, we need to initiate the config and the feature extractor from the Dimitris model.

# %%
from transformers import AutoConfig, Wav2Vec2Processor

# %%
model_name_or_path = "jonatasgrosman/wav2vec2-large-xlsr-53-english"
pooling_mode = "mean"

# %%
# config
config = AutoConfig.from_pretrained(
    model_name_or_path,
    num_labels=num_labels,
    label2id={label: i for i, label in enumerate(label_list)},
    id2label={i: label for i, label in enumerate(label_list)},
    finetuning_task="wav2vec2_clf",
    final_dropout = 0.1
)
print(config.final_dropout)
print(config.hidden_size)
setattr(config, 'pooling_mode', pooling_mode)

# %%
processor = Wav2Vec2Processor.from_pretrained("model/wav2vec2")
target_sampling_rate = processor.feature_extractor.sampling_rate
print(f"The target sampling rate: {target_sampling_rate}")

# %% [markdown]
# # Preprocess Data

# %% [markdown]
# So far, we downloaded, loaded, and split the SER dataset into train and test sets. The instantiated our strategy configuration for using context representations in our classification problem SER. Now, we need to extract features from the audio path in context representation tensors and feed them into our classification model to determine the emotion in the speech.
# 
# Since the audio file is saved in the `.wav` format, it is easy to use **[Librosa](https://librosa.org/doc/latest/index.html)** or others, but we suppose that the format may be in the `.mp3` format in case of generality. We found that the **[Torchaudio](https://pytorch.org/audio/stable/index.html)** library works best for reading in `.mp3` data.
# 
# An audio file usually stores both its values and the sampling rate with which the speech signal was digitalized. We want to store both in the dataset and write a **map(...)** function accordingly. Also, we need to handle the string labels into integers for our specific classification task in this case, the **single-label classification** you may want to use for your **regression** or even **multi-label classification**.
# 
# ---
# 
# åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä¸‹è½½ã€åŠ è½½å¹¶å°† SER æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬è¿˜å®žä¾‹åŒ–äº†ç­–ç•¥é…ç½®ï¼Œä»¥åœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä»ŽéŸ³é¢‘è·¯å¾„ä¸­æå–ç‰¹å¾ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºå¼ é‡ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ†ç±»æ¨¡åž‹ï¼Œä»¥ç¡®å®šè¯­éŸ³ä¸­çš„æƒ…æ„Ÿã€‚
# 
# ç”±äºŽéŸ³é¢‘æ–‡ä»¶ä¿å­˜ä¸º `.wav` æ ¼å¼ï¼Œä½¿ç”¨ **[Librosa](https://librosa.org/doc/latest/index.html)** æˆ–å…¶ä»–å·¥å…·éžå¸¸æ–¹ä¾¿ï¼Œä½†ä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬å‡è®¾éŸ³é¢‘æ–‡ä»¶å¯èƒ½æ˜¯ `.mp3` æ ¼å¼ã€‚ç»è¿‡æµ‹è¯•ï¼Œæˆ‘ä»¬å‘çŽ° **[Torchaudio](https://pytorch.org/audio/stable/index.html)** åº“åœ¨è¯»å– `.mp3` æ•°æ®æ–¹é¢è¡¨çŽ°æœ€ä½³ã€‚
# 
# éŸ³é¢‘æ–‡ä»¶é€šå¸¸åŒ…å«å…¶æ•°å€¼æ•°æ®ä»¥åŠå¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œæ•°å­—åŒ–çš„é‡‡æ ·çŽ‡ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨æ•°æ®é›†ä¸­å­˜å‚¨è¿™ä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼Œå¹¶ç›¸åº”åœ°ç¼–å†™ **map(...)** å‡½æ•°ã€‚æ­¤å¤–ï¼Œåœ¨æœ¬ä»»åŠ¡ä¸­éœ€è¦å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•´æ•°ï¼Œä»¥æ»¡è¶³ç‰¹å®šçš„åˆ†ç±»éœ€æ±‚ï¼ˆå•æ ‡ç­¾åˆ†ç±»ï¼‰ã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥å°†å…¶è°ƒæ•´ä¸ºé€‚ç”¨äºŽ**å›žå½’**æˆ–**å¤šæ ‡ç­¾åˆ†ç±»**çš„ä»»åŠ¡ã€‚

# %%
# def speech_file_to_array_fn(path):
#     # è¿™ä¸ªæ–¹æ³•æ›´å¥½
#     speech_array, sampling_rate = librosa.load(path, sr=target_sampling_rate)
#     return speech_array

import torchaudio

def speech_file_to_array_fn(path):
    speech_array, sampling_rate = torchaudio.load(path)
    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    return speech

def preprocess_function(examples):
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]
    result = processor(speech_list, sampling_rate=target_sampling_rate)
    result["labels"] = examples["PHQ8_Binary"]

    return result

# %%
train_dataset = train_dataset.map(
    preprocess_function,
    batch_size=100,
    batched=True
)
eval_dataset = eval_dataset.map(
    preprocess_function,
    batch_size=100,
    batched=True
)

# %%
len(train_dataset)

# %%
idx = 1134
# print(f"Training input_values: {train_dataset[idx]['input_values']}")
# print(f"Training attention_mask: {train_dataset[idx]['attention_mask']}")
print(f"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['PHQ8_Binary']}")

# %% [markdown]
# Great, now we've successfully read all the audio files, resampled the audio files to 16kHz, and mapped each audio to the corresponding label.

# %% [markdown]
# ## Model
# 
# Before diving into the training part, we need to build our classification model based on the merge strategy.

# %%
from dataclasses import dataclass
from typing import Optional, Tuple
import torch
from transformers.file_utils import ModelOutput


@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None


# %%
import torch
import torch.nn as nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.models.wav2vec2.modeling_wav2vec2 import (
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model
)


class Wav2Vec2ClassificationHead(nn.Module):
    """Head for wav2vec classification task."""

    def __init__(self, config):
        super().__init__()
        self.layer1 = nn.Linear(config.hidden_size, config.hidden_size)
        self.layer2 = nn.Linear(config.hidden_size, config.hidden_size // 2)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size // 2, config.num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)  # Apply dropout before the first layer
        x = self.layer1(x)
        x = torch.tanh(x)  # Activation function after the first layer
        x = self.dropout(x)  # Apply dropout after the first layer

        x = self.layer2(x)
        x = torch.tanh(x)  # Activation function after the second layer
        x = self.dropout(x)  # Apply dropout after the second layer

        x = self.out_proj(x)  # Final linear layer for classification
        return x


class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.pooling_mode = config.pooling_mode
        self.config = config

        self.wav2vec2 = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)

        self.init_weights()

    def freeze_feature_extractor(self):
        self.wav2vec2.feature_extractor._freeze_parameters()
        # self.wav2vec2._freeze_parameters()

    def merged_strategy(
            self,
            hidden_states,
            mode="mean"
    ):
        if mode == "mean":
            outputs = torch.mean(hidden_states, dim=1)
        elif mode == "sum":
            outputs = torch.sum(hidden_states, dim=1)
        elif mode == "max":
            outputs = torch.max(hidden_states, dim=1)[0]
        else:
            raise Exception(
                "The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']")

        return outputs

    def forward(
            self,
            input_values,
            attention_mask=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
            labels=None,
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = outputs[0]
        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SpeechClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


# %% [markdown]
# ## Training
# 
# The data is processed so that we are ready to start setting up the training pipeline. We will make use of ðŸ¤—'s [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:
# 
# - Define a data collator. In contrast to most NLP models, XLSR-Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLSR-Wav2Vec2 requires a special padding data collator, which we will define below
# 
# - Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly
# 
# - Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.
# 
# - Define the training configuration.
# 
# After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech.

# %% [markdown]
# ### Set-up Trainer
# 
# Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81).
# 
# Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.
# Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss.
# 
# ---
# 
# é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆdata collatorï¼‰ã€‚è¯¥æ•°æ®æ•´ç†å™¨çš„ä»£ç æ¥æºäºŽ[è¿™ä¸ªç¤ºä¾‹](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81)ã€‚
# 
# ç®€å•æ¥è¯´ï¼Œä¸Žå¸¸è§çš„æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨ä¼šå¯¹ `input_values` å’Œ `labels` è¿›è¡Œä¸åŒçš„å¤„ç†ï¼Œå› æ­¤å¯¹å®ƒä»¬åˆ†åˆ«åº”ç”¨äº†ä¸åŒçš„å¡«å……å‡½æ•°ï¼ˆåŒæ ·åˆ©ç”¨äº† XLSR-Wav2Vec2 çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰ã€‚è¿™ç§å¤„ç†æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºè¯­éŸ³è¾“å…¥å’Œè¾“å‡ºå±žäºŽä¸åŒçš„æ¨¡æ€ï¼ˆmodalitiesï¼‰ï¼Œå› æ­¤ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„å¡«å……å‡½æ•°æ¥å¤„ç†å®ƒä»¬ã€‚
# 
# ä¸Žå¸¸è§çš„æ•°æ®æ•´ç†å™¨ç±»ä¼¼ï¼Œåœ¨æ ‡ç­¾ä¸­ç”¨ `-100` æ›¿ä»£å¡«å……å€¼ï¼Œè¿™æ ·åœ¨è®¡ç®—æŸå¤±æ—¶ï¼Œè¿™äº›å¡«å……å€¼å°†**ä¸ä¼š**è¢«çº³å…¥è€ƒè™‘ã€‚

# %%
from dataclasses import dataclass
from typing import Dict, List, Optional, Union
import torch

import transformers
from transformers import Wav2Vec2Processor


@dataclass
class DataCollatorCTCWithPadding:
    """
    Data collator that will dynamically pad the inputs received.
    Args:
        processor (:class:`~transformers.Wav2Vec2Processor`)
            The processor used for proccessing the data.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence if provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
        max_length (:obj:`int`, `optional`):
            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
        max_length_labels (:obj:`int`, `optional`):
            Maximum length of the ``labels`` returned list and optionally padding length (see above).
        pad_to_multiple_of (:obj:`int`, `optional`):
            If set will pad the sequence to a multiple of the provided value.
            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
    """

    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [feature["labels"] for feature in features]

        d_type = torch.long if isinstance(label_features[0], int) else torch.float

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        batch["labels"] = torch.tensor(label_features, dtype=d_type)

        return batch

# %%
data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

# %% [markdown]
# Next, the evaluation metric is defined. There are many pre-defined metrics for classification/regression problems, but in this case, we would continue with just **Accuracy** for classification and **MSE** for regression. You can define other metrics on your own.

# %%
import numpy as np
from transformers import EvalPrediction

is_regression = False

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)

    if is_regression:
        return {"mse": ((preds - p.label_ids) ** 2).mean().item()}
    else:
        return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}

# %% [markdown]
# Now, we can load the pretrained XLSR-Wav2Vec2 checkpoint into our classification model with a pooling strategy.

# %%
model = Wav2Vec2ForSpeechClassification.from_pretrained(
    model_name_or_path,
    config=config,
)

model

# %% [markdown]
# The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore.
# Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.
# 
# ---
# 
# XLSR-Wav2Vec2 çš„ç¬¬ä¸€éƒ¨åˆ†ç”±ä¸€ç»„ CNN å±‚ç»„æˆï¼Œç”¨äºŽä»ŽåŽŸå§‹è¯­éŸ³ä¿¡å·ä¸­æå–å…·æœ‰å£°å­¦æ„ä¹‰ä½†ä¸Žä¸Šä¸‹æ–‡æ— å…³çš„ç‰¹å¾ã€‚è¯¥éƒ¨åˆ†æ¨¡åž‹å·²ç»åœ¨é¢„è®­ç»ƒé˜¶æ®µå¾—åˆ°äº†å……åˆ†è®­ç»ƒï¼Œæ­£å¦‚ [è®ºæ–‡](https://arxiv.org/pdf/2006.13979.pdf) ä¸­æ‰€è¿°ï¼Œä¸éœ€è¦å†è¿›è¡Œå¾®è°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°† *ç‰¹å¾æå–* éƒ¨åˆ†ä¸­æ‰€æœ‰å‚æ•°çš„ `requires_grad` è®¾ç½®ä¸º `False`ã€‚

# %%
model.freeze_feature_extractor()

# %% [markdown]
# In a final step, we define all parameters related to training.
# To give more explanation on some of the parameters:
# - `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.
# 
# For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).
# 
# **Note**: If one wants to save the trained models in his/her google drive the commented-out `output_dir` can be used instead.

# %%
# from google.colab import drive

# drive.mount('/gdrive')

# %%
from typing import Any, Dict, Union

import torch
from packaging import version
from torch import nn

from transformers import (
    Trainer,
    is_apex_available,
)

if is_apex_available():
    from apex import amp

if version.parse(torch.__version__) >= version.parse("1.6"):
    _is_native_amp_available = True
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        """
        Perform a training step on a batch of inputs.

        Subclass and override to inject custom behavior.

        Args:
            model (:obj:`nn.Module`):
                The model to train.
            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                argument :obj:`labels`. Check your model's documentation for all accepted arguments.

        Return:
            :obj:`torch.Tensor`: The tensor with training loss on this batch.
        """

        model.train()
        inputs = self._prepare_inputs(inputs)

        with autocast():
            loss = self.compute_loss(model, inputs)
            
        # if self.use_cuda_amp:
        #     print("Use_AMP")
        # with autocast():
        #     loss = self.compute_loss(model, inputs)
        # else:
        #     print("NOT_Use_AMP")
        #     loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        self.scaler.scale(loss).backward()
            
        # if self.use_cuda_amp:
        #     self.scaler.scale(loss).backward()
        # elif self.use_apex:
        #     with amp.scale_loss(loss, self.optimizer) as scaled_loss:
        #         scaled_loss.backward()
        # elif self.deepspeed:
        #     self.deepspeed.backward(loss)
        # else:
        #     loss.backward()

        return loss.detach()


# %%
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="result/wav2vec_binary",
    # output_dir="/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition"
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    gradient_accumulation_steps=1,
    evaluation_strategy="steps",
    num_train_epochs=2.0,
    fp16=True,
    save_steps=25,
    eval_steps=25,
    logging_steps=25,
    learning_rate=1e-4,
    save_total_limit=1
)

# %% [markdown]
# For future use we can create our training script, we do it in a simple way. You can add more on you own.
# 
# Now, all instances can be passed to Trainer and we are ready to start training!

# %%
from transformers import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=processor.feature_extractor,
    optimizers = (optimizer,None)
)

# %%
print(trainer.optimizer)

# %% [markdown]
# ### Training

# %% [markdown]
# Training will take between 10 and 60 minutes depending on the GPU allocated to this notebook.
# 
# In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (right mouse click -> inspect -> Console tab and insert code).

# %% [markdown]
# ```javascript
# function ConnectButton(){
#     console.log("Connect pushed");
#     document.querySelector("#top-toolbar > colab-connect-button").shadowRoot.querySelector("#connect").click()
# }
# setInterval(ConnectButton,60000);
# ```

# %%
trainer.train()

# %% [markdown]
# The training loss goes down and we can see that the Acurracy on the test set also improves nicely. Because this notebook is just for demonstration purposes, we can stop here.
# 
# The resulting model of this notebook has been saved to [m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition](https://huggingface.co/m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition)
# 
# As a final check, let's load the model and verify that it indeed has learned to recognize the emotion in the speech.
# 
# Let's first load the pretrained checkpoint.

# %% [markdown]
# ## Evaluation

# %%
import librosa
from sklearn.metrics import classification_report
from datasets import load_dataset, DownloadMode

test_dataset = load_dataset("csv", data_files={"test": "datasets/output_data/test.csv"}, delimiter="\t", download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)["test"]
test_dataset

# %%
import torch 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

# %%
from transformers import AutoConfig, Wav2Vec2Processor

model_name_or_path = "result/wav2vec_binary/checkpoint-594"
processor_path = "model/wav2vec2"
config = AutoConfig.from_pretrained(model_name_or_path)
processor = Wav2Vec2Processor.from_pretrained(processor_path)
model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)

# %%
import torchaudio

def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch["Audio_Path"])
    speech_array = speech_array.squeeze().numpy()

    batch["speech"] = speech_array
    return batch
    
test_dataset = test_dataset.map(speech_file_to_array_fn)

# %%
features = processor(test_dataset[0]['speech'], sampling_rate=processor.feature_extractor.sampling_rate)
input_values = torch.tensor(features.input_values).to(device)
print(input_values)

# %%
def predict(batch):
    features = processor(batch["speech"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors="pt", padding=True)

    input_values = features.input_values.to(device)

    with torch.no_grad():
        logits = model(input_values).logits 

    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()
    batch["predicted"] = pred_ids
    return batch
    
result = test_dataset.map(predict, batched=True, batch_size=8)

# %%
label_names = [config.id2label[i] for i in range(config.num_labels)]
label_names

# %%
y_true = [name for name in result["PHQ8_Binary"]]
y_pred = result["predicted"]

print(y_true[:5])
print(y_pred[:5])

# %%
def classification_report_with_int_labels(y_true, y_pred, target_names=None):
    """
    Wrapper for sklearn's classification_report that accepts integer labels in `target_names`.
    
    Parameters:
        - y_true: Ground truth (correct) target values.
        - y_pred: Estimated targets as returned by a classifier.
        - target_names: List of class names (int or str). If int, they will be converted to strings.

    Returns:
        - A classification report as a string.
    """
    if target_names is not None:
        # Ensure all target_names are strings
        target_names = [str(label) if isinstance(label, int) else label for label in target_names]
    
    # Generate and return the classification report
    return classification_report(y_true, y_pred, target_names=target_names)
print(classification_report_with_int_labels(y_true, y_pred, target_names=label_names))

# %% [markdown]
# # Prediction

# %%
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from transformers import AutoConfig, Wav2Vec2Processor

import librosa
import IPython.display as ipd
import numpy as np
import pandas as pd

# %%
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path = "m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition"
config = AutoConfig.from_pretrained(model_name_or_path)
processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)
sampling_rate = processor.feature_extractor.sampling_rate
model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)

# %%
def speech_file_to_array_fn(path, sampling_rate):
    speech_array, _sampling_rate = torchaudio.load(path)
    resampler = torchaudio.transforms.Resample(_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    return speech


def predict(path, sampling_rate):
    speech = speech_file_to_array_fn(path, sampling_rate)
    features = processor(speech, sampling_rate=sampling_rate)

    input_values = features.input_values.to(device)
    attention_mask = features.attention_mask.to(device)

    with torch.no_grad():
        logits = model(input_values, attention_mask=attention_mask).logits

    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]
    outputs = [{"Emotion": config.id2label[i], "Score": f"{round(score * 100, 3):.1f}%"} for i, score in enumerate(scores)]
    return outputs


STYLES = """
<style>
div.display_data {
    margin: 0 auto;
    max-width: 500px;
}
table.xxx {
    margin: 50px !important;
    float: right !important;
    clear: both !important;
}
table.xxx td {
    min-width: 300px !important;
    text-align: center !important;
}
</style>
""".strip()

def prediction(df_row):
    path, emotion = df_row["path"], df_row["emotion"]
    df = pd.DataFrame([{"Emotion": emotion, "Sentence": "    "}])
    setup = {
        'border': 2,
        'show_dimensions': True,
        'justify': 'center',
        'classes': 'xxx',
        'escape': False,
    }
    ipd.display(ipd.HTML(STYLES + df.to_html(**setup) + "<br />"))
    speech, sr = torchaudio.load(path)
    speech = speech[0].numpy().squeeze()
    speech = librosa.resample(np.asarray(speech), sr, sampling_rate)
    ipd.display(ipd.Audio(data=np.asarray(speech), autoplay=True, rate=sampling_rate))

    outputs = predict(path, sampling_rate)
    r = pd.DataFrame(outputs)
    ipd.display(ipd.HTML(STYLES + r.to_html(**setup) + "<br />"))

# %%
test = pd.read_csv("/content/data/test.csv", sep="\t")
test.head()

# %%
prediction(test.iloc[0])

# %%
prediction(test.iloc[1])

# %%
prediction(test.iloc[2])


